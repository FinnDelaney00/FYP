# SmartStream Data Platform - Terraform Infrastructure

A complete, production-ready AWS data pipeline built with Terraform, featuring Change Data Capture (CDC), real-time streaming, data transformation, cataloging, and ML inference capabilities.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SMARTSTREAM DATA PIPELINE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  RDS (PG)    â”‚  Source Database
 â”‚  PostgreSQL  â”‚  â”œâ”€ Employee Data
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€ Credentials in Secrets Manager
        â”‚
        â”‚ CDC (Change Data Capture)
        â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     DMS      â”‚  Database Migration Service
 â”‚  Replication â”‚  â”œâ”€ Full Load + CDC
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€ JSON-formatted output
        â”‚
        â”‚ Stream to Kinesis
        â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Kinesis    â”‚  Streaming Backbone
 â”‚ Data Stream  â”‚  â”œâ”€ 2 shards (configurable)
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€ 24h retention
        â”‚
        â”‚ Consume & Deliver
        â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Firehose    â”‚  S3 Delivery
 â”‚   Delivery   â”‚  â”œâ”€ GZIP compression
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€ Low-latency buffering
        â”‚
        â”‚ Write to S3 Raw Zone
        â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                     S3 DATA LAKE                          â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
 â”‚  â”‚   RAW/   â”‚â†’â†’â†’â”‚  TRUSTED/   â”‚â†’â†’â†’â”‚ TRUSTED-ANALYTICSâ”‚  â”‚
 â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                   â”‚
         â”‚ S3 Event       â”‚ EventBridge      â”‚
         â†“                â†“                   â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Lambda   â”‚    â”‚  Glue    â”‚       â”‚  Glue    â”‚
   â”‚Transform â”‚    â”‚ Crawler  â”‚       â”‚ Crawler  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚                   â”‚
        â”‚ Cleanse &      â”‚ Catalog          â”‚ Catalog
        â”‚ Validate       â†“                   â†“
        â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚   GLUE DATA CATALOG         â”‚
        â””â”€â”€â”€â”€â”€â”€â†’   â”‚   (Schema Registry)         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Query
                              â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Athena  â”‚  SQL Analytics
                        â”‚ Workgroupâ”‚  â””â”€ Serverless queries
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚         ML INFERENCE LAYER                    â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
   â”‚  â”‚EventBridgeâ”‚â”€â”€â”€â”€â”€â”€â†’   â”‚   Lambda     â”‚     â”‚
   â”‚  â”‚ Schedule  â”‚  Hourly  â”‚ ML Inference â”‚     â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
   â”‚                                â”‚              â”‚
   â”‚          Reads from TRUSTED/   â”‚              â”‚
   â”‚          Writes to ANALYTICS/  â†“              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

Before deploying this infrastructure, ensure you have:

1. **AWS Account** with appropriate permissions
2. **Terraform** >= 1.5.0 installed ([Download](https://www.terraform.io/downloads))
3. **AWS CLI** configured with credentials ([Setup Guide](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html))
4. **Sufficient AWS Service Quotas**:
   - VPC (1)
   - RDS instances (1)
   - DMS replication instances (1)
   - Kinesis shards (2+)
   - Lambda functions (2)
   - S3 buckets (2)

## ğŸš€ Quick Start

### 1. Clone and Navigate

```bash
cd smartstream-terraform
```

### 2. Initialize Terraform

```bash
terraform init
```

This downloads required providers (AWS, Archive, Random).

### 3. Review and Customize Variables

Edit `terraform.tfvars` (create it if it doesn't exist):

```hcl
# terraform.tfvars
project_name = "smartstream"
env          = "dev"
region       = "eu-north-1"

# Database Configuration
db_name     = "employees"
db_username = "dbadmin"
# db_password is auto-generated if not provided

# Kinesis Configuration
kinesis_shards = 2

# Firehose Configuration (low-latency defaults)
firehose_buffer_interval_seconds = 60  # 1 minute
firehose_buffer_size_mb          = 5   # 5 MB

# CloudWatch Logs
log_retention_days = 7

# RDS Configuration
rds_instance_class    = "db.t3.micro"
rds_allocated_storage = 20

# ML Inference Schedule
ml_schedule_expression = "rate(1 hour)"

# Glue Crawler Schedule
glue_crawler_schedule = "cron(0 2 * * ? *)"  # Daily at 2 AM UTC
```

### 4. Plan Infrastructure

```bash
terraform plan
```

Review the resources that will be created (~50+ resources).

### 5. Deploy Infrastructure

```bash
terraform apply
```

Type `yes` when prompted. **Deployment takes approximately 15-20 minutes**.

### 6. Retrieve Outputs

```bash
terraform output
```

Save important outputs like:
- RDS endpoint
- S3 bucket names
- Kinesis stream name
- Lambda function names
- Athena workgroup name

## ğŸ“Š Post-Deployment Steps

### 1. Populate Source Database

Connect to RDS and create sample data:

```bash
# Get RDS credentials
aws secretsmanager get-secret-value \
  --secret-id $(terraform output -raw rds_secret_arn) \
  --query SecretString --output text | jq .

# Connect to RDS (use endpoint from output)
psql -h <RDS_ENDPOINT> -U dbadmin -d employees

# Create sample table
CREATE TABLE employees (
    id SERIAL PRIMARY KEY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    email VARCHAR(100),
    department VARCHAR(50),
    salary NUMERIC(10,2),
    hire_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

# Insert sample data
INSERT INTO employees (first_name, last_name, email, department, salary, hire_date)
VALUES
    ('John', 'Doe', 'john.doe@example.com', 'Engineering', 95000.00, '2022-01-15'),
    ('Jane', 'Smith', 'jane.smith@example.com', 'Marketing', 78000.00, '2022-03-20'),
    ('Bob', 'Johnson', 'bob.johnson@example.com', 'Sales', 82000.00, '2021-11-10');
```

### 2. Start DMS Replication Task

The DMS task is created but not auto-started. Start it manually:

```bash
aws dms start-replication-task \
  --replication-task-arn $(terraform output -raw dms_replication_task_arn) \
  --start-replication-task-type start-replication
```

Monitor the task:

```bash
aws dms describe-replication-tasks \
  --filters Name=replication-task-arn,Values=$(terraform output -raw dms_replication_task_arn)
```

### 3. Verify Data Flow

**Check Kinesis Stream:**
```bash
aws kinesis describe-stream \
  --stream-name $(terraform output -raw kinesis_stream_name)
```

**Check S3 Raw Zone:**
```bash
aws s3 ls s3://$(terraform output -raw data_lake_bucket_name)/raw/ --recursive
```

**Check S3 Trusted Zone (after Lambda transform):**
```bash
aws s3 ls s3://$(terraform output -raw data_lake_bucket_name)/trusted/ --recursive
```

### 4. Run Glue Crawlers

Catalog the data for Athena queries:

```bash
# Crawl trusted zone
aws glue start-crawler --name $(terraform output -raw glue_trusted_crawler_name)

# Crawl analytics zone
aws glue start-crawler --name $(terraform output -raw glue_analytics_crawler_name)

# Check crawler status
aws glue get-crawler --name $(terraform output -raw glue_trusted_crawler_name)
```

### 5. Query Data with Athena

Navigate to Athena console or use AWS CLI:

```bash
# List tables in Glue catalog
aws glue get-tables --database-name $(terraform output -raw glue_database_name)

# Run a query (replace table_name with actual table from Glue)
aws athena start-query-execution \
  --query-string "SELECT * FROM table_name LIMIT 10" \
  --query-execution-context Database=$(terraform output -raw glue_database_name) \
  --result-configuration OutputLocation=s3://$(terraform output -raw athena_results_bucket_name)/results/
```

### 6. Test CDC (Change Data Capture)

Make changes to the source database:

```sql
-- Insert new record
INSERT INTO employees (first_name, last_name, email, department, salary, hire_date)
VALUES ('Alice', 'Williams', 'alice.williams@example.com', 'HR', 70000.00, '2023-05-01');

-- Update existing record
UPDATE employees SET salary = 98000.00 WHERE id = 1;

-- Delete record
DELETE FROM employees WHERE id = 3;
```

DMS will capture these changes and stream them through the pipeline!

### 7. Monitor the Pipeline

**CloudWatch Dashboard:**
```bash
# Get dashboard URL
terraform output quick_start_commands
```

**Check Lambda Logs:**
```bash
# Transform Lambda
aws logs tail /aws/lambda/$(terraform output -raw lambda_transform_function_name) --follow

# ML Lambda
aws logs tail /aws/lambda/$(terraform output -raw lambda_ml_function_name) --follow
```

## ğŸ”§ Customization Guide

### Scaling Kinesis

To handle higher throughput, increase shard count:

```hcl
# terraform.tfvars
kinesis_shards = 4  # Each shard: 1 MB/s in, 2 MB/s out
```

Then apply:
```bash
terraform apply
```

### Adjusting Lambda Memory/Timeout

Edit `lambda_transform.tf` or `lambda_ml.tf`:

```hcl
resource "aws_lambda_function" "transform" {
  # ...
  timeout     = 600  # 10 minutes
  memory_size = 1024 # 1 GB
}
```

### Changing ML Inference Schedule

```hcl
# terraform.tfvars
ml_schedule_expression = "rate(30 minutes)"  # Run every 30 minutes
# or
ml_schedule_expression = "cron(0 */6 * * ? *)"  # Every 6 hours
```

### Adding Custom Lambda Logic

Replace placeholder logic in:
- `lambdas/transform/lambda_function.py` - Data transformation
- `lambdas/ml/lambda_function.py` - ML inference

After changes:
```bash
terraform apply  # Re-zips and deploys Lambda code
```

## ğŸ“ File Structure

```
smartstream-terraform/
â”œâ”€â”€ versions.tf           # Terraform and provider versions
â”œâ”€â”€ providers.tf          # Provider configurations
â”œâ”€â”€ variables.tf          # Input variables
â”œâ”€â”€ locals.tf             # Computed local values
â”œâ”€â”€ outputs.tf            # Output values
â”œâ”€â”€ networking.tf         # VPC, subnets, security groups
â”œâ”€â”€ s3.tf                 # S3 buckets (data lake, Athena results)
â”œâ”€â”€ secrets.tf            # Secrets Manager for RDS credentials
â”œâ”€â”€ rds.tf                # PostgreSQL database
â”œâ”€â”€ kinesis.tf            # Kinesis Data Stream
â”œâ”€â”€ dms.tf                # DMS replication instance and tasks
â”œâ”€â”€ firehose.tf           # Kinesis Firehose delivery stream
â”œâ”€â”€ lambda_transform.tf   # Transform Lambda function
â”œâ”€â”€ lambda_ml.tf          # ML inference Lambda function
â”œâ”€â”€ glue.tf               # Glue Data Catalog and Crawlers
â”œâ”€â”€ athena.tf             # Athena workgroup
â”œâ”€â”€ iam.tf                # IAM roles and policies (least-privilege)
â”œâ”€â”€ cloudwatch.tf         # CloudWatch monitoring and alarms
â”œâ”€â”€ lambdas/
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ lambda_function.py  # Transform handler
â”‚   â””â”€â”€ ml/
â”‚       â””â”€â”€ lambda_function.py  # ML inference handler
â””â”€â”€ README.md             # This file
```

## ğŸ’° Cost Estimation

**Monthly costs for default configuration (eu-north-1):**

| Service | Configuration | Estimated Cost |
|---------|--------------|----------------|
| RDS PostgreSQL | db.t3.micro, 20GB | ~$15 |
| DMS | dms.t3.micro | ~$13 |
| Kinesis Data Stream | 2 shards | ~$22 |
| Kinesis Firehose | 1 GB/day | ~$1 |
| Lambda | 100 invocations/day | ~$1 |
| S3 Storage | 10 GB | ~$0.25 |
| NAT Gateway | 1 instance | ~$32 |
| Glue Crawler | Daily runs | ~$0.44 |
| Athena | 1 GB scanned/day | ~$0.15 |
| **TOTAL** | | **~$85/month** |

**Cost Optimization Tips:**
- Use VPC endpoints to eliminate NAT Gateway charges (~$32/month savings)
- Reduce Kinesis retention period (24h â†’ 1h)
- Use lifecycle policies to move old data to Glacier
- Stop DMS when not actively replicating

## ğŸ”’ Security Features

- âœ… **Encryption at Rest**: S3 (SSE-S3), RDS (encrypted storage)
- âœ… **Encryption in Transit**: SSL/TLS for RDS, HTTPS for S3/Kinesis
- âœ… **Secrets Management**: RDS credentials in AWS Secrets Manager
- âœ… **Least-Privilege IAM**: Each service has minimal required permissions
- âœ… **Network Isolation**: Private subnets for RDS, DMS, Lambda
- âœ… **S3 Public Access Block**: Enabled on all buckets
- âœ… **VPC Security Groups**: Restrictive ingress/egress rules

## ğŸ› Troubleshooting

### DMS Task Fails to Start

**Check:**
1. RDS is in "available" state
2. Logical replication is enabled (`rds.logical_replication = 1`)
3. Security groups allow DMS â†’ RDS connectivity
4. Secrets Manager has correct credentials

**Solution:**
```bash
aws dms test-connection \
  --replication-instance-arn $(terraform output -raw dms_replication_instance_arn) \
  --endpoint-arn $(terraform output -raw dms_source_endpoint_arn)
```

### Lambda Transform Not Triggering

**Check:**
1. S3 event notification is configured
2. Lambda has permission to be invoked by S3
3. Files are being written to `/raw/` prefix

**Solution:**
```bash
aws lambda get-policy \
  --function-name $(terraform output -raw lambda_transform_function_name)
```

### Glue Crawler Finds No Tables

**Check:**
1. Data exists in S3 trusted zone
2. Data is in supported format (JSON, CSV, Parquet)
3. Crawler has S3 read permissions

**Solution:**
```bash
aws s3 ls s3://$(terraform output -raw data_lake_bucket_name)/trusted/ --recursive
```

### High Kinesis Iterator Age

**Symptom:** Firehose falling behind Kinesis stream

**Solution:**
- Increase Firehose buffering
- Reduce Kinesis retention period
- Add more Firehose delivery streams

## ğŸ§¹ Cleanup

To destroy all infrastructure:

```bash
# Stop DMS task first (optional, but cleaner)
aws dms stop-replication-task \
  --replication-task-arn $(terraform output -raw dms_replication_task_arn)

# Empty S3 buckets (required before destroy)
aws s3 rm s3://$(terraform output -raw data_lake_bucket_name) --recursive
aws s3 rm s3://$(terraform output -raw athena_results_bucket_name) --recursive

# Destroy infrastructure
terraform destroy
```

Type `yes` when prompted. **Destruction takes approximately 10-15 minutes**.

## ğŸ“š Data Flow Summary

1. **Source**: RDS PostgreSQL contains employee data
2. **Ingestion**: DMS captures changes (full load + CDC) and streams to Kinesis
3. **Streaming**: Kinesis Data Stream acts as buffer/backbone
4. **Delivery**: Firehose reads from Kinesis, compresses, and writes to S3 `/raw/`
5. **Transformation**: Lambda triggers on S3 events, cleanses data, writes to `/trusted/`
6. **Cataloging**: Glue Crawlers discover schema and update Data Catalog
7. **Analytics**: Athena queries cataloged data using SQL
8. **ML**: Scheduled Lambda reads `/trusted/`, runs inference, writes to `/trusted-analytics/`

## ğŸ¯ Next Steps

- [ ] Set up SNS email notifications for alerts
- [ ] Implement data quality checks in Transform Lambda
- [ ] Add actual ML model to ML Lambda
- [ ] Create Athena views for common queries
- [ ] Implement data retention policies
- [ ] Add CICD pipeline for Lambda deployments
- [ ] Enable VPC Flow Logs for network monitoring
- [ ] Implement AWS Backup for RDS
- [ ] Add AWS WAF for API protection (if exposing endpoints)
- [ ] Create data documentation in Glue Data Catalog

## ğŸ“– References

- [AWS DMS Documentation](https://docs.aws.amazon.com/dms/)
- [Kinesis Data Streams](https://docs.aws.amazon.com/kinesis/)
- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/)
- [Amazon Athena](https://docs.aws.amazon.com/athena/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)

## ğŸ“ License

This infrastructure code is provided as-is for educational and development purposes.

## ğŸ¤ Support

For issues or questions:
1. Check the Troubleshooting section above
2. Review Terraform plan output for errors
3. Check AWS CloudWatch Logs for service-specific issues
4. Review AWS service quotas and limits

---

**Built with â¤ï¸ using Terraform and AWS**
